---
title: "Assignment of STAT319"
author: "Tonghui Xu (96973487)"
date: "3/20/2020"
output: pdf_document
---

Question 1

Part one:
```{r}
#set up 
library(foreign)
kidata = read.dta("kidiq.dta")
# explore and summary the data
model_1<- lm(kid_score ~ mom_iq,data=kidata)
plot(kid_score~mom_iq,data=kidata)
abline(model_1$coef,lty=1,lwd=2,col="blue")
summary(model_1)
par(mfrow=c(2,2))
plot(model_1)
```

1.The p-value asssociated with the variable mom_iq is highly statistically singnificant, at "< 2e-16". Where $< 2e-16= 2\times 10^{-16}$, so that the p-value is effectively zero. This p-value comes form a t-test and the null hypothesis is $\hat\beta_1=0$. Because p-value is smaller than 0.05, we have a very strong evidenve against the null hypothesis that mom_iq has no effect on kid_score.

2.Conclusion of the assumptions check:

There are few potential outliers(numbered as 136, 286 and 7) in the four plots.

$\bullet$ Residuals vs Fitted plot shows a curve shape in the residuals. So the linearity assumption does not meet.

$\bullet$ The Normal Q-Q plot shows the residuals follow a reasonbly normal distribution. 

$\bullet$ The Scale-location plot shows there is not a horizontal line with randomly spread points so the variance is not constant. Hence, the residuals do not meet the equal variance assumption.

$\bullet$ The residuals vs Leverage shows these potential outliers are not influential to the regression results. It follows the horizontal line mostly so it meets the independence assumption.

3. model_1 is : $kidScore=\beta_0 + momIq*\beta_1$

Since model one does not meet the linearity assumption, we can try to improve the model by addding a quadratic term on it .
```{r}
model_2 <- lm(kid_score~mom_iq+ I(mom_iq^2),data=kidata)
summary(model_2)
#plot(kid.data2,which=1)
plot(kid_score~mom_iq,data=kidata)
o <- order(kidata$mom_iq)
lines(kidata$mom_iq[o],fitted(model_1)[o],lwd=2,col="red")
lines(kidata$mom_iq[o],fitted(model_2)[o],lwd=2,col="blue")
legend("topright",legend=c("model_1","model_2"),col=c("red","blue"),lwd=4,text.col="black")
par(mfrow=c(2,2))
plot(model_2)
```
4. The p-value of the quadratic term is 0.000767, which mean the the quadratic term of mom's IQ is statistically significant to predict the kid's congnitive score.

5.Conclusion of the assumptions check:

There are few potential outliers(numbered as 136, 324 and 7) in the four plots.

$\bullet$ Residuals vs Fitted plot shows the residuals meet the linearity assumption.

$\bullet$ The Normal Q-Q plot shows the residuals follow a reasonbly normal distribution. 

$\bullet$ The Scale-location plot shows there is approximately a horizontal line with randomly spread points so the variance is constant. Hence, the residuals meet the equal variance assumption.

$\bullet$ The residuals vs Leverage shows the residuals follow the horizontal line mostly so it meets the independence assumption. Also, there is no high influence points.

6. Model_2 is : $kidScore= -99.03 + 3.08* momIq - 0.01*momIq + \epsilon \space where \space \epsilon \space  is \space iid \space Normal(0,\sigma^2).$

7. Interpret model_2:

$\bullet$ The intercept is -99.03, which means kid's congnitive score is -99.03 when mom's IQ is zero. This does not make any sense in reality. 

$\bullet$ There is a positive relationship between mom's IQ and kid's congnitive score. Kid's congnitive score is higher when mom's IQ is higher. However,the negative$\beta2$ coefficient(-0.01) indicates an decrease of kid's congnitive score that starts slowly and "decelerates" as mom's IQ increases.
   
Part 2
```{r}
boxplot(kid_score~mom_hs,data=kidata)
```
The box plot shows the congnitive score of these kids whose mom graduated from high school is generally higher than the congnitive score of these kids whose mom not graduated from high school.

Fit the "addictive" model(no interaction)
```{r}
model_3 <- lm(kid_score~mom_iq+mom_hs,data=kidata)
summary(model_3)
par(mfrow=c(2,2))
plot(model_3)
```

Fit the "intersection" model
```{r}
model_4 <- lm(kid_score~mom_iq*mom_hs,data=kidata)
summary(model_4)
par(mfrow=c(2,2))
plot(model_4)
#plot of the interaction
par(mfrow=c(1,1))
plot(kid_score ~ mom_iq, data = kidata, pch = substr(mom_hs, 0, 1), 
  xlim = c(70, 140), ylim = c(20, 140)) 
abline(model_4$coef[1], model_4$coef[2], col = "brown") #baseline, No graduate
abline(model_4$coef[1] + model_4$coef[3], model_4$coef[2] + model_4$coef[4], 
  col = "purple")  
legend("bottomright",legend=c("non graduated"," graduated"),col=c("brown","purple"),lwd=4,text.col="black")
```
1. Check the assumptions:

   $\bullet$ Residuals vs Fitted plot shows the residuals meet the linearity assumption.

   $\bullet$ The Normal Q-Q plot shows the residuals follow a reasonbly normal distribution. 

   $\bullet$ The Scale-location plot shows there is approximately a horizontal line with randomly spread points so the variance is constant. Hence, the residuals meet the equal variance assumption.

   $\bullet$ The residuals vs Leverage shows the residuals follow the horizontal line mostly so it meets the independence assumption. Also, there is no high influence points.


2.Interpretation of coefficients:

The "interaction term" test: the p-value for the interaction term is 0.002(<0.05), we have strong evidence to reject the null hypothesis of there being no difference in the slope of the two lines. So the interaction term is statistically signiticant and we are correct to fit two lines with different slopes. Likewise, there is a significant difference between the intercepts since the p-value is 0.0009(<0.05). The baseline slope that measures the effect of mom's IQ for not graduated from high school is statistically significant(p-value is $1.84\times 10^{-10}$).

2.For the baseline regression("No graduated)

$\bullet$ the intercept is -11.48 and the slope is 0.97;

$\bullet$ $kidScore = -11.48 + 0.97*momIq +\epsilon$

3.For the other regression("graduated from high school")

$\bullet$ the intercept is -11.48 + 51.27 = 39.79 and the slope is 0.97 â€“ 0.48 = 0.49;

$\bullet$ $kidScore = 39.79 + 0.49*momIq + \epsilon$

3.There is a positive relationship between mom's IQ and kid's congnitive score. For every unit increase in mom's IQ, kid's congnitive score increase by 0.49 units.

4. The basline(brown line) has a larger slope than the other regression line(purple line), but a lower intercept. When mom's IQ is greater than around 105, the congnitive score of the kids whose mom not graduated from high school is higher than the congnitive score of the kids whose moms graduated from high school. When mom's IQ is smaller than around 105, the congnitive score of the kids whose mom not graduated from high school is lower than the congnitive score of the kids whose moms graduated from high school.

Question 2

```{r}
library(s20x)
bursary<- read.csv("bursary.csv")
model_5 <- lm(pass.rate~decile,data=bursary)
trendscatter(pass.rate~decile,data=bursary)
par(mfrow=c(2,2))
plot(model_5)
summary(model_5)
```

```{r}
model_6 <- lm(pass.rate~decile+I(decile^2), data=bursary)
par(mfrow=c(2,2))
plot(model_6)
summary(model_6)
```

```{r}
model_7 <- lm(pass.rate ~ log(decile),data=bursary)
par(mfrow=c(2,2))
plot(model_7)
par(mfrow=c(1,1))
trendscatter(pass.rate ~log(decile),data=bursary)
summary(model_7)
```

1.We fit a simple linear model and it shows there is a non-constant scatter.

2.Conclusion of the assumptions of the simple linear model :

Residuals vs Fitted plot shows the residuals do not meet the linearity assumption since there is a deviation from linearity for some fitted values.

Normal Q-Q shows only small deviations from normality (dashed line), so the residuals meet the normality assumption.

The Scale-location plot shows there is not a horizontal line with randomly spread points so the variance is not constant. Hence, the residuals do not meet the equal variance assumption.

The residuals vs Leverage shows these potential outliers are not influential to the regression results. It follows the horizontal line mostly so it meets the independence assumption.

3.We add a quadratic term and it improves the outcome of the residual plots and the residuals meet the assumption of linearity and constant varariance. But it is not helpful to give us a constant scatter.

4.We make a linear log model to fit the data and we find out it gives us a constant scatter. Also, the residual plots look better than the previous model.

5.Check the assumptions of the linear log model: 

Residuals vs Fitted plot shows the residuals meet the linearity assumption.

Normal Q-Q shows only small deviations from normality (dashed line), so the residuals meet the normality assumption.

The Scale-location plot shows there is a horizontal line with randomly spread points so the variance is constant. Hence, the residuals meet the equal variance assumption.

The residuals vs Leverage shows these potential outliers are not influential to the regression results and the residuals follow the horizontal line mostly so it meets the independence
assumption.

6.The best model is : $$pass.rate = 18.69\% + 28.93\% \times log(decile) + \epsilon$$

7.Interpretation of the coefficients:

Log(decile): For one percent increase in decile rating of the school would result in a 28.93\% increase in the pass rate.
Intercept: When ln(Decile) = 0, average pass rate is around 18.69%.

Question 3

```{r}
# read in and look at the data
library(s20x)
data(course.df)
#making the variable Pass numeric 
course.df$Pass <- as.numeric(course.df$Pass) 
course.df$Pass <- course.df$Pass-1
# make model one with all variables
course_model1 <- glm(Pass~Gender+Assign+Test+Stage1,family = binomial, data=course.df)
summary(course_model1)
```

The p-value of stage1B and stage1C are larger than 0.05, which means the variable "stage" is not statistically significant to predict if the student pass the course. So we do not need "satge" in the model.

```{r}
#model 2
course_model2 <- glm(Pass~Gender+Assign+Test,family = binomial, data=course.df)
summary(course_model2)
```

The p-value of GenderMale is larger than 0.05, which means the variable "GenderMale" is not statistically significant to predict if the student pass the course. So we do not need "GenderMale" in the model.

```{r}
#model 3
course_model2 <- glm(Pass~Assign+Test,family = binomial, data=course.df)
summary(course_model2)
```
The p-value of Assignment and Test are both very smaller than 0.05, So they are statistically significant to be in this model. Hence, the best model can be written as:
$$ Pass =-16.053 0+ 0.7002*Assignment + 0.7869 * Test + \epsilon$$ where $$\space  \epsilon \sim N(0,\sigma^2)$$

$\bullet$ We add assignments and test marks together to make a new variable "marks",which can be used to predict whether the students pass the course or not.

```{r}
marks <- course.df$Assign + course.df$Test
model_course4 <- glm(Pass~marks,family = binomial, data=course.df)
summary(model_course4)
plot(Pass~marks,data = course.df,pch=19)
marks = 0:40
linear.comb <- model_course4$coef[1] + model_course4$coef[2] * marks
probs <- exp(linear.comb)/(1+exp(linear.comb))
lines(marks,probs,col="red",lwd=2)
```

The probability of a student pass the course is:
$$ P = \frac{exp(\beta_0 + \beta_1X)}{1+exp(\beta_0 + \beta_1X)} $$
$$=\frac{exp(-16.0245 + 0.7366*X)}{1+exp(-16.0245 + 0.7366*X)}$$
The p-value of marks is $2.8\times 10^{-7}$, which is much smaller than 0.05. So the variable "marks" is statistically significant. In this case, we can conclude that whether a student pass the course is related a lot to their total marks for assignment and test. There is a positive relationship between the Pass and marks of assignments and tests. So the probabilty of a student pass the course increases as the total marks of assignments and tests increases. Also, student are more likely to pass the course if they get more than 25 total marks for the assignments and tests; student are less likely to pass the course if they get less than 20 total marks for the assignments and tests.



